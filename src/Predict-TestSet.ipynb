{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) Prepare the main test set CSV from TXT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>sent.no</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentences</th>\n",
       "      <th>neutral</th>\n",
       "      <th>generalization</th>\n",
       "      <th>exemplification</th>\n",
       "      <th>attribution</th>\n",
       "      <th>conditional</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>156</td>\n",
       "      <td>nlh.47.1.626118_nonotes.txt</td>\n",
       "      <td>To this end, one of the main merits of Merleau...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>207</td>\n",
       "      <td>ahr.2016.121.2.437_nonotes.txt</td>\n",
       "      <td>In their response they chastised her for her u...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  section  sent.no                        filename  \\\n",
       "0       e      156     nlh.47.1.626118_nonotes.txt   \n",
       "1       b      207  ahr.2016.121.2.437_nonotes.txt   \n",
       "\n",
       "                                           sentences  neutral  generalization  \\\n",
       "0  To this end, one of the main merits of Merleau...        1               0   \n",
       "1  In their response they chastised her for her u...        1               0   \n",
       "\n",
       "   exemplification  attribution  conditional  ID  \n",
       "0                0            0            0   1  \n",
       "1                0            0            0   2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Gen_Sentences_Annotated_All_Final_Processed.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get sentences from txt(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpString(text):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "\n",
    "\n",
    "def get_top_bottom_sentences(fname):\n",
    "    \"\"\"\n",
    "    Given a filename, return a list of sentences that include the first ~1500 words and last ~1000 words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Filename for the txt\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list, list\n",
    "        The first is a list of intro sentences; second is a list of conclusion sentences.\n",
    "    \"\"\"\n",
    "    with open(PATH+fname, 'r') as f:\n",
    "        txt = f.read()\n",
    "    \n",
    "    clean_txt = cleanUpString(txt)\n",
    "        \n",
    "    sents = sent_tokenize(clean_txt)\n",
    " \n",
    "    ## First 1500 words ##\n",
    "    intro_sentences = []\n",
    "    counter1 = 0\n",
    "    for sent in sents:\n",
    "        intro_sentences.append(sent)\n",
    "        # Keep track of number of words covered, and break out of the loop if it crosses 1500.\n",
    "        words = word_tokenize(sent)\n",
    "        counter1 += len(words)\n",
    "        if counter1 > 1500:\n",
    "            break\n",
    "\n",
    "    ## Final 1000 words ##\n",
    "    conclusion_sentences = []\n",
    "    counter2 = 0\n",
    "    for sent in reversed(sents):\n",
    "        conclusion_sentences.append(sent)\n",
    "        # Keep track of number of words covered, and break out of the loop if it crosses 1000.\n",
    "        words = word_tokenize(sent)\n",
    "        counter2 += len(words)\n",
    "        if counter2 > 1000:\n",
    "            break\n",
    "            \n",
    "    return intro_sentences, conclusion_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 230 files.\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "map_fname_sents = defaultdict(dict) # {fname1: {'INTRO': [list-of-sents], 'CONCLUSION': [list-of-sents]}, \n",
    "                                    #  fname2: {..} ..}\n",
    "PATH = '/Users/sunyambagga/Desktop/txtLAB/Sentence-Prediction/txt_nonotes/'\n",
    "\n",
    "c = 0\n",
    "for fname in os.listdir(PATH):\n",
    "    if '.DS_Store' in fname:\n",
    "        continue\n",
    "    \n",
    "    intro_sents, conclusion_sents = get_top_bottom_sentences(fname)\n",
    "    \n",
    "    map_fname_sents[fname]['INTRO'] = intro_sents\n",
    "    map_fname_sents[fname]['CONCLUSION'] = conclusion_sents\n",
    "    \n",
    "    c += 1\n",
    "\n",
    "print(\"Processed {} files.\".format(c))\n",
    "print(len(map_fname_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 16816 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Write to CSV:\n",
    "total_sents = 0\n",
    "\n",
    "input_dict = {'sentences': [], 'filename': [], 'section': [], 'ID': []}\n",
    "\n",
    "for fname in map_fname_sents:\n",
    "    for key in map_fname_sents[fname]: # key is either 'INTRO' or 'CONCLUSION'\n",
    "        for sent_number, sent in enumerate(map_fname_sents[fname][key]):\n",
    "            total_sents += 1\n",
    "            \n",
    "            ID = fname[:-4]+'__'+key+'__'+ str(sent_number) # Create unique ID using fname, INTRO/CON, sent number\n",
    "            input_dict['sentences'].append(sent)\n",
    "            input_dict['filename'].append(fname)\n",
    "            input_dict['section'].append(key)\n",
    "            input_dict['ID'].append(ID)\n",
    "            \n",
    "print(\"Total of {} sentences.\".format(total_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16816, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>filename</th>\n",
       "      <th>section</th>\n",
       "      <th>ID</th>\n",
       "      <th>neutral</th>\n",
       "      <th>generalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 These typically confident directives encapsu...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...   \n",
       "1  1 These typically confident directives encapsu...   \n",
       "\n",
       "                        filename section                                   ID  \\\n",
       "0  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__0   \n",
       "1  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__1   \n",
       "\n",
       "  neutral generalization  \n",
       "0                         \n",
       "1                         "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(input_dict)\n",
    "df['neutral'] = \"\"\n",
    "df['generalization'] = \"\"\n",
    "\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df.to_csv('../data/TEST_SET.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2) Get CNN predictions on test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Load the CNN model trained on 3.4k instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator, BasicIterator\n",
    "from allennlp.data import Instance\n",
    "from allennlp.models import Model\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer # for ELMo\n",
    "\n",
    "import models\n",
    "import data_reader\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*labels', '*tags'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocabulary\n",
    "vocab = Vocabulary.from_files(\"../saved_model/vocabulary\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LARGE ELMo..\n",
      "Pre-trained ELMo loaded..\n"
     ]
    }
   ],
   "source": [
    "# Re-instantiate the CNN model:\n",
    "label_cols = [\"generalization\", \"neutral\"] # Ordering is important\n",
    "batch_size = 64\n",
    "num_filters = 100\n",
    "filter_sizes = (2,3,4,5)\n",
    "num_classes = 2\n",
    "\n",
    "word_embeddings = train.load_elmo_embeddings(large=True)\n",
    "\n",
    "# CNN encoder\n",
    "encoder = CnnEncoder(embedding_dim=word_embeddings.get_output_dim(),\n",
    "                     num_filters=num_filters,\n",
    "                     ngram_filter_sizes=filter_sizes)\n",
    "\n",
    "# Feedforward:\n",
    "classifier_feedforward = nn.Linear(encoder.get_output_dim(), num_classes)\n",
    "\n",
    "\n",
    "model = models.Classifier(vocab=vocab,\n",
    "                          word_embeddings=word_embeddings,\n",
    "                          encoder=encoder,\n",
    "                          classifier_feedforward=classifier_feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model:\n",
    "with open(\"../saved_model/cnn_elmo.th\", 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Predict on the test set (16.8k instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator, cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return out_dict[\"class_probabilities\"]\n",
    "    \n",
    "    def predict(self, dataset: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(dataset, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator,\n",
    "                                   total=self.iterator.get_num_batches(dataset))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)\n",
    "    \n",
    "def make_predictions(model, vocab, test_dataset, batch_size, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Runs the given 'model' on the given 'test_dataset' & returns predictions.\n",
    "    \"\"\"\n",
    "    # iterate over the dataset without changing its order\n",
    "    seq_iterator = BasicIterator(batch_size)\n",
    "    seq_iterator.index_with(vocab)\n",
    "\n",
    "    predictor = Predictor(model, seq_iterator, cuda_device=0 if use_gpu else -1)\n",
    "    preds = predictor.predict(test_dataset)    \n",
    "    return preds\n",
    "\n",
    "def map_id_prediction(pred_probs, test_dataset):\n",
    "    \"\"\"\n",
    "    Maps the ID to the corresponding prediction.\n",
    "    \n",
    "    'pred_probs' is predicted probabilities.\n",
    "    Returns a dictionary with key = ID | value = prediction\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for prediction, sample in zip(pred_probs, test_dataset):\n",
    "        ID = sample.fields['ID'].metadata\n",
    "        if prediction[0] >= 0.5: # because order is ['generalization', 'neutral']\n",
    "            out[ID] = 'generalization'\n",
    "        else:\n",
    "            out[ID] = 'neutral'        \n",
    "    return out\n",
    "\n",
    "def tokenizer(x: str):\n",
    "    return [w.text for w in SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16816it [00:22, 747.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset:\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "reader = data_reader.GeneralizationDatasetReader(tokenizer=tokenizer, token_indexers={\"tokens\": token_indexer},\n",
    "                                                 label_cols=label_cols)\n",
    "\n",
    "DATA_ROOT = Path(\"../data/\")\n",
    "test_fname = './TEST_SET.csv'\n",
    "test_dataset = reader.read(file_path=DATA_ROOT / test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 43 with text: \n",
      " \t\t[WHEN, ABEL, TASMAN, SET, OFF, ON, HIS, voyage, to, discover, the, fabled, Zuijdlandt, ,, or, South,\n",
      "\t\tLand, ,, he, carried, with, him, instructions, to, take, possession, of, all, continents, and,\n",
      "\t\tislands, which, you, should, discover, ,, call, at, or, set, foot, on, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
      " \t ID: MetadataField (print field.metadata to see specific information). \n",
      " \t label: ArrayField with shape: (2,) and dtype: <class 'numpy.float32'>. \n",
      "\n",
      "Label:  {'array': array([nan, nan], dtype=object), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n"
     ]
    }
   ],
   "source": [
    "# Preview test set (labels should be empty)\n",
    "print(test_dataset[0])\n",
    "print(\"Label: \", vars(test_dataset[0]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 263/263 [1:22:27<00:00, 18.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Predict:\n",
    "preds = make_predictions(model, vocab, test_dataset, batch_size) # Note that 'preds' is of the shape (number of samples, 2) - the columns represent the probabilities for the two classes ['generalization', 'neutral']\n",
    "\n",
    "# Map it to IDs:\n",
    "id_pred = map_id_prediction(preds, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16816, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>filename</th>\n",
       "      <th>section</th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 These typically confident directives encapsu...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__1</td>\n",
       "      <td>generalization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...   \n",
       "1  1 These typically confident directives encapsu...   \n",
       "\n",
       "                        filename section                                   ID  \\\n",
       "0  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__0   \n",
       "1  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__1   \n",
       "\n",
       "       prediction  \n",
       "0         neutral  \n",
       "1  generalization  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/TEST_SET.csv')\n",
    "\n",
    "df['prediction'] = df['ID'].map(id_pred)\n",
    "df.drop(['neutral', 'generalization'], axis=1, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df.to_csv('../predictions/TEST_SET_predictions.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16816, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>filename</th>\n",
       "      <th>section</th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 These typically confident directives encapsu...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__1</td>\n",
       "      <td>generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stepping ashore, frequently without pausing fo...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was not done, at least in the first place...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__3</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tasman s instructions stipulated that he could...</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes.txt</td>\n",
       "      <td>INTRO</td>\n",
       "      <td>ahr.2016.121.1.17_nonotes__INTRO__4</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  WHEN ABEL TASMAN SET OFF ON HIS voyage to disc...   \n",
       "1  1 These typically confident directives encapsu...   \n",
       "2  Stepping ashore, frequently without pausing fo...   \n",
       "3  This was not done, at least in the first place...   \n",
       "4  Tasman s instructions stipulated that he could...   \n",
       "\n",
       "                        filename section                                   ID  \\\n",
       "0  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__0   \n",
       "1  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__1   \n",
       "2  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__2   \n",
       "3  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__3   \n",
       "4  ahr.2016.121.1.17_nonotes.txt   INTRO  ahr.2016.121.1.17_nonotes__INTRO__4   \n",
       "\n",
       "       prediction  \n",
       "0         neutral  \n",
       "1  generalization  \n",
       "2         neutral  \n",
       "3         neutral  \n",
       "4         neutral  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check:\n",
    "t = pd.read_csv('../predictions/TEST_SET_predictions.csv')\n",
    "print(t.shape)\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txtlab-allen",
   "language": "python",
   "name": "txtlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
